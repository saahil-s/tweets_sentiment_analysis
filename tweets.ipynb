{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vogrCA5aYPtQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import sklearn\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import string\n",
        "import re # helps you filter urls\n",
        "from IPython.display import display, Latex, Markdown\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PmFK0GgYPtU"
      },
      "source": [
        "# Classifying tweets [100 points]\n",
        "\n",
        "In this problem, you will be analyzing Twitter data extracted using [this](https://dev.twitter.com/overview/api) api. The data contains tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`\n",
        "\n",
        "For every tweet, there are two pieces of information:\n",
        "- `screen_name`: the Twitter handle of the user tweeting and\n",
        "- `text`: the content of the tweet.\n",
        "\n",
        "The tweets have been divided into two parts - train and test available to you in CSV files. For train, both the `screen_name` and `text` attributes were provided but for test, `screen_name` is hidden.\n",
        "\n",
        "The overarching goal of the problem is to \"predict\" the political inclination (Republican/Democratic) of the Twitter user from one of his/her tweets. The ground truth (i.e., true class labels) is determined from the `screen_name` of the tweet as follows\n",
        "- `realDonaldTrump, mike_pence, GOP` are Republicans\n",
        "- `HillaryClinton, timkaine, TheDemocrats` are Democrats\n",
        "\n",
        "Thus, this is a binary classification problem.\n",
        "\n",
        "The problem proceeds in three stages:\n",
        "- **Text processing (25%)**: We will clean up the raw tweet text using the various functions offered by the [nltk](http://www.nltk.org/genindex.html) package.\n",
        "- **Feature construction (25%)**: In this part, we will construct bag-of-words feature vectors and training labels from the processed text of tweets and the `screen_name` columns respectively.\n",
        "- **Classification (50%)**: Using the features derived, we will use [sklearn](http://scikit-learn.org/stable/modules/classes.html) package to learn a model which classifies the tweets as desired.\n",
        "\n",
        "You will use two new python packages in this problem: `nltk` and `sklearn`, both of which should be available with anaconda. However, NLTK comes with many corpora, toy grammars, trained models, etc, which have to be downloaded manually. This assignment requires NLTK's stopwords list, POS tagger, and WordNetLemmatizer. Install them using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wQGjuhliYPtV",
        "outputId": "8179d3ac-7d2f-4837-e5e9-e82a13f9f84b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\saahi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\saahi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\saahi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\saahi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "# Verify that the following commands work for you, before moving on.\n",
        "\n",
        "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
        "stopwords=nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTqEaBw9YPtZ"
      },
      "source": [
        "Let's begin!\n",
        "\n",
        "## A. Text Processing [25 points]\n",
        "\n",
        "You first task to fill in the following function which processes and tokenizes raw text. The generated list of tokens should meet the following specifications:\n",
        "1. The tokens must all be in lower case.\n",
        "2. The tokens should appear in the same order as in the raw text.\n",
        "3. The tokens must be in their lemmatized form. If a word cannot be lemmatized (i.e, you get an exception), simply catch it and ignore it. These words will not appear in the token list.\n",
        "4. The tokens must not contain any punctuations. Punctuations should be handled as follows: (a) Apostrophe of the form `'s` must be ignored. e.g., `She's` becomes `she`. (b) Other apostrophes should be omitted. e.g, `don't` becomes `dont`. (c) Words must be broken at the hyphen and other punctuations.\n",
        "5. The tokens must not contain any part of a url.\n",
        "\n",
        "Part of your work is to figure out a logical order to carry out the above operations. You may find `string.punctuation` useful, to get hold of all punctuation symbols. Look for [regular expressions](https://docs.python.org/3/library/re.html) capturing urls in the text. Your tokens must be of type `str`. Use `nltk.word_tokenize()` for tokenization once you have handled punctuation in the manner specified above.\n",
        "\n",
        "You would want to take a look at the `lemmatize()` function [here](https://www.nltk.org/_modules/nltk/stem/wordnet.html).\n",
        "In order for `lemmatize()` to give you the root form for any word, you have to provide the context in which you want to lemmatize through the `pos` parameter: `lemmatizer.lemmatize(word, pos=SOMEVALUE)`. The context should be the part of speech (POS) for that word. The good news is you do not have to manually write out the lexical categories for each word because [nltk.pos_tag()](https://www.nltk.org/book/ch05.html) will do this for you. Now you just need to use the results from `pos_tag()` for the `pos` parameter.\n",
        "However, you can notice the POS tag returned from `pos_tag()` is in different format than the expected pos by `lemmatizer`.\n",
        "> pos\n",
        "(Syntactic category): n for noun files, v for verb files, a for adjective files, r for adverb files.\n",
        "\n",
        "You need to map these pos appropriately. `nltk.help.upenn_tagset()` provides description of each tag returned by `pos_tag()`.\n",
        "\n",
        "\n",
        "\n",
        "## Q1 (15 points):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B5IMtsQoYPta",
        "outputId": "23d1f6a5-5b3a-4e1b-c8cf-c833e0f17913"
      },
      "outputs": [],
      "source": [
        "# Convert part of speech tag from nltk.pos_tag to word net compatible format\n",
        "# Simple mapping based on first letter of return tag to make grading consistent\n",
        "# Everything else will be considered noun 'n'\n",
        "posMapping = {\n",
        "# \"First_Letter by nltk.pos_tag\":\"POS_for_lemmatizer\"\n",
        "    \"N\":'n',\n",
        "    \"V\":'v',\n",
        "    \"J\":'a',\n",
        "    \"R\":'r'\n",
        "}\n",
        "# 14% credits\n",
        "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
        "    \"\"\" Normalizes case and handles punctuation\n",
        "    Inputs:\n",
        "        text: str: raw text\n",
        "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
        "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
        "    Outputs:\n",
        "        list(str): tokenized text\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "    text = re.sub(url_pattern, \"\", text)\n",
        "\n",
        "    if text.find(\"'s\") > 0:\n",
        "        text = text.replace(\"'s\", \"\")\n",
        "\n",
        "    if text.find(\"-\") > 0:\n",
        "        text = text.replace(\"-\", \" \")\n",
        "\n",
        "    if text.find(\"'\") > 0:\n",
        "        text = text.replace(\"'\", \"\")\n",
        "\n",
        "    for i in text:\n",
        "        if i in string.punctuation:\n",
        "            text = text.replace(i, \" \")\n",
        "\n",
        "    tokenized_text = nltk.word_tokenize(text)\n",
        "\n",
        "    pos_tag_list = nltk.pos_tag(tokenized_text)\n",
        "    \n",
        "    newList = []\n",
        "    for pair in pos_tag_list:\n",
        "        string1, string2 = pair\n",
        "        if \"N\" == string2[0]:\n",
        "            newList.append((string1, 'n'))\n",
        "        elif \"V\" == string2[0]:\n",
        "            newList.append((string1, 'v'))\n",
        "        elif \"J\" == string2[0]:\n",
        "            newList.append((string1, 'a'))\n",
        "        elif \"R\" == string2[0]:\n",
        "            newList.append((string1, 'r'))\n",
        "        else:\n",
        "            newList.append((string1, 'n'))\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for pair in newList:\n",
        "        string1, string2 = pair\n",
        "        lemmatized_tokens.append(lemmatizer.lemmatize(string1.lower(), pos=string2))\n",
        "    \n",
        "    # print(\"my answer:\", lemmatized_tokens)\n",
        "    return lemmatized_tokens\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bspITcPiYPte"
      },
      "source": [
        "You can test the above function as follows. Try to make your test strings as exhaustive as possible. Some checks are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UrEzaolBYPtf",
        "outputId": "5f52df89-dcda-4e95-c34d-4ea2fec56ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['im', 'do', 'well', 'how', 'about', 'you']\n",
            "['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
            "['be', 'have', 'do', 'language', 'city', 'mice']\n",
            "['it', 'hilarious', 'check', 'it', 'out']\n",
            "['see', 'it', 'sunday', 'morning', 'at', '8', '30a', 'on', 'rtv6', 'and', 'our', 'rtv6', 'app', 'http', '…']\n"
          ]
        }
      ],
      "source": [
        "# 1 point credit\n",
        "print(process(\"I'm doing well! How about you?\"))\n",
        "# print(\"Answer:\", ['im', 'do', 'well', 'how', 'about', 'you'])\n",
        "\n",
        "print(process(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"))\n",
        "# print(\"Answer:\", ['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence'])\n",
        "\n",
        "print(process(\"been had done languages cities mice\"))\n",
        "# print(\"Answer:\", ['be', 'have', 'do', 'language', 'city', 'mice'])\n",
        "\n",
        "print(process(\"It's hilarious. Check it out http://t.co/dummyurl\"))\n",
        "# print(\"Answer:\", ['it', 'hilarious', 'check', 'it', 'out'])\n",
        "\n",
        "print(process(\"See it Sunday morning at 8:30a on RTV6 and our RTV6 app. http:…\"))\n",
        "# print(\"Answer:\", ['see', 'it', 'sunday', 'morning', 'at', '8', '30a', 'on', 'rtv6', 'and', 'our', 'rtv6', 'app', 'http', '…'])\n",
        "# Here '…' is a special unicode character not in string.punctuation and it is still present in processed text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylcws_-LYPti"
      },
      "source": [
        "\n",
        "\n",
        "## Q2 (10 points):\n",
        "\n",
        "You will now use the `process()` function we implemented to convert the pandas dataframe we just loaded from tweets_train.csv file. Your function should be able to handle any data frame which contains a column called `text`. The data frame you return should replace every string in `text` with the result of `process()` and retain all other columns as such. Do not change the order of rows/columns. Before writing `process_all()`, load the data into a DataFrame and look at its format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aEONT9fXYPti"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>screen_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GOP</td>\n",
              "      <td>RT @GOPconvention: #Oregon votes today. That m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TheDemocrats</td>\n",
              "      <td>RT @DWStweets: The choice for 2016 is clear: W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>Trump's calling for trillion dollar tax cuts f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>.@TimKaine's guiding principle: the belief tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>timkaine</td>\n",
              "      <td>Glad the Senate could pass a #THUD / MilCon / ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      screen_name                                               text\n",
              "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
              "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
              "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
              "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
              "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
        "display(tweets.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SM7hJBdOYPtl"
      },
      "outputs": [],
      "source": [
        "# 9 points credits\n",
        "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
        "    \"\"\" process all text in the dataframe using process() function.\n",
        "    Inputs\n",
        "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
        "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
        "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
        "    Outputs\n",
        "        pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
        "                        the output from process() function. Other columns are unaffected.\n",
        "    \"\"\"\n",
        "\n",
        "    df['text'] = df['text'].apply(process)\n",
        "\n",
        "    return df\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lXQP6CAiYPto"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      screen_name                                               text\n",
            "0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
            "1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
            "2  HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
            "3  HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
            "4        timkaine  [glad, the, senate, could, pass, a, thud, milc...\n"
          ]
        }
      ],
      "source": [
        "# test your code\n",
        "# 1 point credit\n",
        "processed_tweets = process_all(tweets)\n",
        "print(processed_tweets.head())\n",
        "\n",
        "# data = {'text': ['Hello all I\\'m living', 'This world hasn\\'t been the same', 'Amber is this python or not?']}\n",
        "# df = pd.DataFrame(data)\n",
        "# print(df.head())\n",
        "# print(process_all(df))\n",
        "\n",
        "\n",
        "#       screen_name                                               text\n",
        "# 0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
        "# 1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
        "# 2  HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
        "# 3  HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
        "# 4        timkaine  [glad, the, senate, could, pass, a, thud, milc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>screen_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GOP</td>\n",
              "      <td>[rt, gopconvention, oregon, vote, today, that,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TheDemocrats</td>\n",
              "      <td>[rt, dwstweets, the, choice, for, 2016, be, cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>[trump, call, for, trillion, dollar, tax, cut,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>[timkaine, guide, principle, the, belief, that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>timkaine</td>\n",
              "      <td>[glad, the, senate, could, pass, a, thud, milc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17293</th>\n",
              "      <td>mike_pence</td>\n",
              "      <td>[rt, govpencein, cast, a, line, w, firstladyin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17294</th>\n",
              "      <td>mike_pence</td>\n",
              "      <td>[rt, indiana, edc, indiana, be, at, record, em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17295</th>\n",
              "      <td>mike_pence</td>\n",
              "      <td>[rt, indiana, edc, indy, will, have, a, differ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17296</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>[tune, in, now, to, watch, joebiden, hit, the,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17297</th>\n",
              "      <td>GOP</td>\n",
              "      <td>[rt, chrisyounggop, were, launch, a, leadright...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17298 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          screen_name                                               text\n",
              "0                 GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
              "1        TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
              "2      HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
              "3      HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
              "4            timkaine  [glad, the, senate, could, pass, a, thud, milc...\n",
              "...               ...                                                ...\n",
              "17293      mike_pence  [rt, govpencein, cast, a, line, w, firstladyin...\n",
              "17294      mike_pence  [rt, indiana, edc, indiana, be, at, record, em...\n",
              "17295      mike_pence  [rt, indiana, edc, indy, will, have, a, differ...\n",
              "17296  HillaryClinton  [tune, in, now, to, watch, joebiden, hit, the,...\n",
              "17297             GOP  [rt, chrisyounggop, were, launch, a, leadright...\n",
              "\n",
              "[17298 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsBiunCIYPtr"
      },
      "source": [
        "## B. Feature Construction [25 points]\n",
        "\n",
        "The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words TF-IDF feature vector. But before that, as you may have guessed, the number of possible words is prohibitively large and not all of them may be useful for our classification task. We need to determine which words to retain, and which to omit. A common heuristic is to construct a frequency distribution of words in the corpus and prune out the head and tail of the distribution. The intuition of the above operation is as follows. Very common words (i.e. stopwords) add almost no information regarding similarity of two pieces of text. Similarly with very rare words. NLTK has a list of in-built stop words which is a good substitute for head of the distribution. We will consider a word rare if it occurs only in a single document (row) in whole of `tweets_train.csv`.\n",
        "\n",
        "## Q3 (15 points):\n",
        "\n",
        "Construct a sparse matrix of features for each tweet with the help of `sklearn.feature_extraction.text.TfidfVectorizer` (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). You need to pass a parameter `min_df=2` to filter out the words occuring only in one document in the whole training set. Remember to ignore the stop words as well. You must leave other optional parameters (e.g., `vocab`, `norm`, etc) at their default values. But you may need to use parameters like `lowercase` and `tokenizer` to handle `processed_tweets` that is a `list` of tokens (not raw text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8NQ9wVm3YPt0"
      },
      "outputs": [],
      "source": [
        "# 14 points credits\n",
        "def create_features(processed_tweets, stop_words):\n",
        "    \"\"\" creates the feature matrix using the processed tweet text\n",
        "    Inputs:\n",
        "        processed_tweets: pd.DataFrame: processed tweets read from train/test csv file, containing the column 'text'\n",
        "        stop_words: list(str): stop_words by nltk stopwords (after processing)\n",
        "    Outputs:\n",
        "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
        "            we need this to tranform test tweets in the same way as train tweets\n",
        "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
        "    \"\"\"\n",
        "    def tokenize(text):\n",
        "        return text\n",
        "\n",
        "    stop_word_list = list(stop_words)\n",
        "    sorted_stop_word_list = sorted(stop_word_list)\n",
        "\n",
        "    # Constructing vectorizer\n",
        "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, tokenizer=tokenize, stop_words=sorted_stop_word_list, min_df=2)\n",
        "    tfidf_matrix = vectorizer.fit_transform(processed_tweets['text'])\n",
        "\n",
        "    # Constructing the CSR\n",
        "    tfidf_matrix_csr = scipy.sparse.csr.csr_matrix(tfidf_matrix)\n",
        "\n",
        "    # return vectorizer and CSR\n",
        "    return vectorizer, tfidf_matrix_csr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XXYtqEyvYPt4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "c:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  warnings.warn(\n",
            "C:\\Users\\saahi\\AppData\\Local\\Temp\\ipykernel_27368\\1690778641.py:23: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  tfidf_matrix_csr = scipy.sparse.csr.csr_matrix(tfidf_matrix)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(TfidfVectorizer(lowercase=False, min_df=2,\n",
              "                 stop_words=['a', 'about', 'above', 'after', 'again', 'against',\n",
              "                             'ain', 'all', 'an', 'and', 'any', 'aren', 'arent',\n",
              "                             'at', 'be', 'because', 'before', 'below', 'between',\n",
              "                             'both', 'but', 'by', 'can', 'couldn', 'couldnt',\n",
              "                             'd', 'didn', 'didnt', 'do', 'doesn', ...],\n",
              "                 tokenizer=<function create_features.<locals>.tokenize at 0x00000277F3D9EFC0>),\n",
              " <17298x8167 sparse matrix of type '<class 'numpy.float64'>'\n",
              " \twith 169355 stored elements in Compressed Sparse Row format>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# execute this code\n",
        "# 1 point credit\n",
        "# It is recommended to process stopwords according to our data cleaning rules\n",
        "processed_stopwords = set(np.concatenate([process(word) for word in stopwords]))\n",
        "(tfidf, X) = create_features(processed_tweets, processed_stopwords)\n",
        "# Ignore warning\n",
        "tfidf, X\n",
        "# Output (should be similar):\n",
        "# (TfidfVectorizer(lowercase=False, min_df=2,\n",
        "#                  stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
        "#                              'ain', 'all', 'an', 'and', 'any', 'aren', 'arent',\n",
        "#                              'at', 'be', 'because', 'before', 'below', 'between',\n",
        "#                              'both', 'but', 'by', 'can', 'couldn', 'couldnt',\n",
        "#                              'd', 'didn', 'didnt', 'do', 'doesn', ...},\n",
        "#                  tokenizer=<function create_features.<locals>.<lambda> at 0x7fd4002a6700>),\n",
        "#  <17298x8114 sparse matrix of type '<class 'numpy.float64'>'\n",
        "#  \twith 170355 stored elements in Compressed Sparse Row format>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WQJih7SYPt6"
      },
      "source": [
        "\n",
        "\n",
        "## Q4 (10%):\n",
        "\n",
        "Also for each tweet, assign a class label (0 or 1) using its `screen_name`. Use 0 for realDonaldTrump, mike_pence, GOP and 1 for the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>screen_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GOP</td>\n",
              "      <td>[rt, gopconvention, oregon, vote, today, that,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TheDemocrats</td>\n",
              "      <td>[rt, dwstweets, the, choice, for, 2016, be, cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>[trump, call, for, trillion, dollar, tax, cut,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>[timkaine, guide, principle, the, belief, that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>timkaine</td>\n",
              "      <td>[glad, the, senate, could, pass, a, thud, milc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17293</th>\n",
              "      <td>mike_pence</td>\n",
              "      <td>[rt, govpencein, cast, a, line, w, firstladyin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17294</th>\n",
              "      <td>mike_pence</td>\n",
              "      <td>[rt, indiana, edc, indiana, be, at, record, em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17295</th>\n",
              "      <td>mike_pence</td>\n",
              "      <td>[rt, indiana, edc, indy, will, have, a, differ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17296</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>[tune, in, now, to, watch, joebiden, hit, the,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17297</th>\n",
              "      <td>GOP</td>\n",
              "      <td>[rt, chrisyounggop, were, launch, a, leadright...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17298 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          screen_name                                               text\n",
              "0                 GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
              "1        TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
              "2      HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
              "3      HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
              "4            timkaine  [glad, the, senate, could, pass, a, thud, milc...\n",
              "...               ...                                                ...\n",
              "17293      mike_pence  [rt, govpencein, cast, a, line, w, firstladyin...\n",
              "17294      mike_pence  [rt, indiana, edc, indiana, be, at, record, em...\n",
              "17295      mike_pence  [rt, indiana, edc, indy, will, have, a, differ...\n",
              "17296  HillaryClinton  [tune, in, now, to, watch, joebiden, hit, the,...\n",
              "17297             GOP  [rt, chrisyounggop, were, launch, a, leadright...\n",
              "\n",
              "[17298 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(processed_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bLtDPaInYPt7",
        "outputId": "1d10cd7e-cc42-4bee-eedb-34d9d486031f"
      },
      "outputs": [],
      "source": [
        "# 9 point credits\n",
        "def create_labels(processed_tweets):\n",
        "    \"\"\" creates the class labels from screen_name\n",
        "    Inputs:\n",
        "        processed_tweets: pd.DataFrame: tweets read from train file, containing the column 'screen_name'\n",
        "    Outputs:\n",
        "        numpy.ndarray(int): dense binary numpy array of class labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Get a list of the screen names\n",
        "    screen_names = processed_tweets['screen_name'].tolist()\n",
        "\n",
        "    # Define class labels\n",
        "    class_labels_mapping = {\n",
        "        'GOP': 0,\n",
        "        'realDonaldTrump': 0,\n",
        "        'mike_pence': 0,\n",
        "        'HillaryClinton': 1,\n",
        "        'timkaine': 1,\n",
        "        'TheDemocrats': 1\n",
        "    }\n",
        "\n",
        "    # Map screen names to class labels\n",
        "    class_labels = [class_labels_mapping[screen_name] for screen_name in screen_names]\n",
        "\n",
        "    # Convert class labels to Pandas Series\n",
        "    class_labels_series = pd.Series(class_labels, name='screen_name')\n",
        "\n",
        "    return class_labels_series.to_numpy().astype(np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z49a4djKYPt-",
        "outputId": "c6dac151-7d26-4296-cdbb-c745bf8a2b19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 1, 0])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# execute this code\n",
        "# 1 point credit\n",
        "y = create_labels(processed_tweets)\n",
        "y\n",
        "# 0        0\n",
        "# 1        1\n",
        "# 2        1\n",
        "# 3        1\n",
        "# 4        1\n",
        "#         ..\n",
        "# 17293    0\n",
        "# 17294    0\n",
        "# 17295    0\n",
        "# 17296    1\n",
        "# 17297    0\n",
        "# Name: screen_name, Length: 17298, dtype: int32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5STbtJYSYPuA"
      },
      "source": [
        "## C. Classification [50 points]\n",
        "\n",
        "And finally, we are ready to put things together and learn a model for the classification of tweets. The classifier you will be using is [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (Support Vector Machine).\n",
        "\n",
        "At the heart of SVMs is the concept of kernel functions, which determines how the similarity/distance between two data points in computed. `sklearn`'s SVM provides four kernel functions: `linear`, `poly`, `rbf`, `sigmoid` (details [here](http://scikit-learn.org/stable/modules/svm.html#svm-kernels)) but you can also implement your own distance function and pass it as an argument to the classifier.\n",
        "\n",
        "Through the various functions you implement in this part, you will be able to learn a classifier, score a classifier based on how well it performs, use it for prediction tasks and compare it to a baseline.\n",
        "\n",
        "Specifically, you will carry out the following tasks (Q5-9) in order:\n",
        "\n",
        "1. Implement and evaluate a simple baseline classifier MajorityLabelClassifier.\n",
        "2. Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}.\n",
        "3. Implement the `evaluate_classifier()` function which scores a classifier based on accuracy of a given dataset.\n",
        "4. Implement `best_model_selection()` to perform cross-validation by calling `learn_classifier()` and `evaluate_classifier()` for different folds and determine which of the four kernels performs the best.\n",
        "5. Go back to `learn_classifier()` and fill in the best kernel.\n",
        "\n",
        "\n",
        "## Q5 (10 points):\n",
        "\n",
        "To determine whether your classifier is performing well, you need to compare it to a baseline classifier. A baseline is generally a simple or trivial classifier and your classifier should beat the baseline in terms of a performance measure such as accuracy. Implement a classifier called `MajorityLabelClassifier` that always predicts the class equal to **mode** of the labels (i.e., the most frequent label) in training data. Part of the code is done for you. Implement the `fit` and `predict` methods. Initialize your classifier appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xUBpZ6_NYPuB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training accuracy: 0.5001734304543878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "c:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  warnings.warn(\n",
            "C:\\Users\\saahi\\AppData\\Local\\Temp\\ipykernel_27368\\1690778641.py:23: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  tfidf_matrix_csr = scipy.sparse.csr.csr_matrix(tfidf_matrix)\n"
          ]
        }
      ],
      "source": [
        "# Skeleton of MajorityLabelClassifier is consistent with other sklearn classifiers\n",
        "# 8 points credits\n",
        "class MajorityLabelClassifier():\n",
        "    \"\"\"\n",
        "    A classifier that predicts the mode of training labels\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize your parameter here\n",
        "        \"\"\"\n",
        "        y_mode = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Implement fit by taking training data X and their labels y and finding the mode of y\n",
        "        i.e. store your learned parameter\n",
        "        \"\"\"\n",
        "        self.y_mode = scipy.stats.mode(y)[0]\n",
        "        return self.y_mode\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Implement to give the mode of training labels as a prediction for each data instance in X\n",
        "        return labels\n",
        "        \"\"\"\n",
        "        return [self.y_mode] * X.shape[0]\n",
        "\n",
        "# 2 points credits\n",
        "# Report the accuracy of your classifier by comparing the predicted label of each example to its true label\n",
        "baselineClf = MajorityLabelClassifier()\n",
        "# Use fit and predict methods to get predictions and compare it with the true labels y\n",
        "\n",
        "# Using fit\n",
        "\n",
        "y_fit = baselineClf.fit(X, y)\n",
        "\n",
        "# Using predict\n",
        "test_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
        "\n",
        "processed_test_tweets = process_all(test_tweets)\n",
        "processed_stopwords = set(np.concatenate([process(word) for word in stopwords]))\n",
        "(tfidf_test, X_test) = create_features(processed_test_tweets, processed_stopwords)\n",
        "\n",
        "y_predict = baselineClf.predict(X_test)\n",
        "\n",
        "# print(training accuracy) should give 0.5001734304543878\n",
        "score = np.mean(y)\n",
        "print(\"training accuracy:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZldwD1JYPuD"
      },
      "source": [
        "\n",
        "## Q6 (10 points):\n",
        "\n",
        "Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}. Stick to default values for any other optional parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k1zAYuAsQNB"
      },
      "source": [
        "Hint: Check https://scikit-learn.org/stable/modules/svm.html#svm-kernels on how to use sklearn.svm.SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WGpfOK2EYPuD"
      },
      "outputs": [],
      "source": [
        "# 9 points credits\n",
        "\n",
        "def learn_classifier(X_train, y_train, kernel):\n",
        "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
        "    Inputs:\n",
        "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features(), X\n",
        "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_labels(), y\n",
        "        kernel: str: kernel function to be used with classifier. [linear|poly|rbf|sigmoid]\n",
        "    Outputs:\n",
        "        sklearn.svm.SVC: classifier learnt from data\n",
        "    \"\"\"\n",
        "\n",
        "    # Create SVM classifier\n",
        "    clf = sklearn.svm.SVC(kernel=kernel)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iOwIKVuvYPuF"
      },
      "outputs": [],
      "source": [
        "# execute code\n",
        "# 1 point credit\n",
        "classifier = learn_classifier(X, y, 'linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDsVwZuNYPuK"
      },
      "source": [
        "\n",
        "\n",
        "## Q7 (10 points):\n",
        "\n",
        "Now that we know how to learn a classifier, the next step is to evaluate it, ie., characterize how good its classification performance is. This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model.\n",
        "\n",
        "There are two questions that should now come to your mind:\n",
        "1. **What data to use?**\n",
        "    - **Validation Data**: The data used to evaluate a classifier is called **validation data** (or hold-out data), and it is usually different from the data used for training. The model or hyperparameter with the best performance in the held out data is chosen. This approach is relatively fast and simple but vulnerable to biases found in validation set.\n",
        "    - **Cross-validation**: This approach divides the dataset in $k$ groups (so, called k-fold cross-validation). One of group is used as test set for evaluation and other groups as training set. The model or hyperparameter with the best average performance across all k folds is chosen. For this question you will perform 4-fold cross validation to determine the best kernel. We will keep all other hyperparameters default for now. This approach provides robustness toward biasness in validation set. However, it takes more time.\n",
        "    \n",
        "2. **And what metric?** There are several evaluation measures available in the literature (e.g., accuracy, precision, recall, F-1,etc) and different fields have different preferences for specific metrics due to different goals. We will go with accuracy. According to wiki, **accuracy** of a classifier measures the fraction of all data points that are correctly classified by it; it is the ratio of the number of correct classifications to the total number of (correct and incorrect) classifications. `sklearn.metrics` provides a number of performance metrics.\n",
        "\n",
        "Now, implement the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tvLsjxqKYPuL"
      },
      "outputs": [],
      "source": [
        "# 9 points credits\n",
        "def evaluate_classifier(classifier, X_validation, y_validation):\n",
        "    \"\"\" evaluates a classifier based on a supplied validation data\n",
        "    Inputs:\n",
        "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
        "        X_validation: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
        "        y_validation: numpy.ndarray(int): dense binary vector of class labels\n",
        "    Outputs:\n",
        "        double: accuracy of classifier on the validation data\n",
        "    \"\"\"\n",
        "    y_predict = classifier.predict(X_validation)\n",
        "\n",
        "    return sklearn.metrics.accuracy_score(y_validation, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pmdMkFIzYPuN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9547346514047866\n"
          ]
        }
      ],
      "source": [
        "# test your code by evaluating the accuracy on the training data\n",
        "# 1 point credit\n",
        "accuracy = evaluate_classifier(classifier, X, y)\n",
        "print(accuracy)\n",
        "# should give around 0.9545034107989363"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYYIADjZYPuP"
      },
      "source": [
        "\n",
        "\n",
        "## Q8 (10 points):\n",
        "\n",
        "Now it is time to decide which kernel works best by using the cross-validation technique. Write code to split the training data into 4-folds (75% training and 25% validation) by shuffling randomly. For each kernel, record the average accuracy for all folds and determine the best classifier. Since our dataset is balanced (both classes are in almost equal propertion), `sklearn.model_selection.KFold` [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) can be used for cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nq5AlMoUYPuQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KFold(n_splits=4, random_state=1, shuffle=True)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)\n",
        "kf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og6hzUPPYPuT"
      },
      "source": [
        "Then use the following code to determine which classifier is the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oJAjQyNKYPuT",
        "outputId": "7abee0c6-97cf-42dc-ae11-9723b55a9845"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_kernel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#Test your code\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m best_kernel \u001b[38;5;241m=\u001b[39m best_model_selection(kf, X, y)\n\u001b[0;32m     39\u001b[0m best_kernel\n",
            "Cell \u001b[1;32mIn[19], line 28\u001b[0m, in \u001b[0;36mbest_model_selection\u001b[1;34m(kf, X, y)\u001b[0m\n\u001b[0;32m     25\u001b[0m     X_train, X_val \u001b[38;5;241m=\u001b[39m X[train_index], X[val_index]\n\u001b[0;32m     26\u001b[0m     y_train, y_val \u001b[38;5;241m=\u001b[39m y[train_index], y[val_index]\n\u001b[1;32m---> 28\u001b[0m     clf \u001b[38;5;241m=\u001b[39m learn_classifier(X_train, y_train, kernel)\n\u001b[0;32m     29\u001b[0m     accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m evaluate_classifier(clf, X_val, y_val)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
            "Cell \u001b[1;32mIn[14], line 17\u001b[0m, in \u001b[0;36mlearn_classifier\u001b[1;34m(X_train, y_train, kernel)\u001b[0m\n\u001b[0;32m     14\u001b[0m clf \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39msvm\u001b[38;5;241m.\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39mkernel)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clf\n",
            "File \u001b[1;32mc:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
            "File \u001b[1;32mc:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:371\u001b[0m, in \u001b[0;36mBaseLibSVM._sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    357\u001b[0m kernel_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_kernels\u001b[38;5;241m.\u001b[39mindex(kernel)\n\u001b[0;32m    359\u001b[0m libsvm_sparse\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    361\u001b[0m (\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    364\u001b[0m     dual_coef_data,\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 371\u001b[0m ) \u001b[38;5;241m=\u001b[39m libsvm_sparse\u001b[38;5;241m.\u001b[39mlibsvm_sparse_train(\n\u001b[0;32m    372\u001b[0m     X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    373\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    374\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices,\n\u001b[0;32m    375\u001b[0m     X\u001b[38;5;241m.\u001b[39mindptr,\n\u001b[0;32m    376\u001b[0m     y,\n\u001b[0;32m    377\u001b[0m     solver_type,\n\u001b[0;32m    378\u001b[0m     kernel_type,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma,\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef0,\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    386\u001b[0m     sample_weight,\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu,\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_size,\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon,\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinking),\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability),\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m    393\u001b[0m     random_seed,\n\u001b[0;32m    394\u001b[0m )\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32msklearn\\svm\\_libsvm_sparse.pyx:219\u001b[0m, in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\saahi\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:25\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_cs_matrix\u001b[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"base matrix class for compressed row- and column-oriented matrices\"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg1, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     26\u001b[0m         _data_matrix\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m issparse(arg1):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 10 points credits\n",
        "def best_model_selection(kf, X, y):\n",
        "    \"\"\"\n",
        "    Select the kernel giving best results using k-fold cross-validation.\n",
        "    Other parameters should be left default.\n",
        "    Input:\n",
        "    kf (sklearn.model_selection.KFold): kf object defined above\n",
        "    X (scipy.sparse.csr.csr_matrix): training data\n",
        "    y (array(int)): training labels\n",
        "    Return:\n",
        "    best_kernel (string)\n",
        "    \"\"\"\n",
        "    # kernel_accuracies = {'linear': 0, 'rbf': 0, 'poly': 0, 'sigmoid': 0}\n",
        "    best_kernel = ''\n",
        "    best_accuracy = 0\n",
        "    for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
        "        # Use the documentation of KFold cross-validation to split ..\n",
        "        # training data and test data from create_features() and create_labels()\n",
        "        # call learn_classifer() using training split of kth fold\n",
        "        # evaluate on the test split of kth fold\n",
        "        # record avg accuracies and determine best model (kernel)\n",
        "        #return best kernel as string\n",
        "        accuracy = 0\n",
        "        for train_index, val_index in kf.split(X):\n",
        "            X_train, X_val = X[train_index], X[val_index]\n",
        "            y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "            clf = learn_classifier(X_train, y_train, kernel)\n",
        "            accuracy += evaluate_classifier(clf, X_val, y_val)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_kernel = kernel\n",
        "\n",
        "    return best_kernel\n",
        "\n",
        "#Test your code\n",
        "best_kernel = best_model_selection(kf, X, y)\n",
        "best_kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yho_UYX7YPuX"
      },
      "source": [
        "\n",
        "## Q9 (10 points)\n",
        "\n",
        "We're almost done! It's time to write a nice little wrapper function that will use our model to classify unlabeled tweets from tweets_test.csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qth7DPekYPuY"
      },
      "outputs": [],
      "source": [
        "# 8points credits\n",
        "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
        "    \"\"\" predicts class labels for raw tweet text\n",
        "    Inputs:\n",
        "        tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
        "        classifier: sklearn.svm.SVC: classifier learned\n",
        "        unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
        "    Outputs:\n",
        "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
        "    \"\"\"\n",
        "    \n",
        "    # svm_clf = learn_classifier(X, y, 'poly')\n",
        "    # accuracy = evaluate_classifier(svm_clf, X, y)\n",
        "    # process_all(unlabeled_tweets)\n",
        "\n",
        "    new_df = process_all(unlabeled_tweets)\n",
        "    matrix = tfidf.transform(new_df['text']) #ncreate a matrix which contains tfidf of each word\n",
        "    return classifier.predict(matrix) # predict the label\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeuZhVR2OhyV"
      },
      "source": [
        "Did your SVM classifier perform better than the baseline (while evaluating with training data)? Explain in 1-2 sentences how you reached this conclusion.\n",
        "\n",
        "Below, I have compared the accuracy from the baseline and my poly SVM classifier. The poly one is better as its accuracy is higher than the linear one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9VTWMbRWsQND"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1]\n",
            "poly is better\n"
          ]
        }
      ],
      "source": [
        "classifier = learn_classifier(X, y, 'poly')\n",
        "unlabeled_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
        "y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)\n",
        "print(y_pred)\n",
        "\n",
        "poly_score = evaluate_classifier(classifier, X, y)\n",
        "\n",
        "if (score < poly_score):\n",
        "    print(\"poly is better\")\n",
        "else:\n",
        "    print(\"linear is better\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1cd19471928a1bd1d711eeee745b6e15cf888a1d547a12676dd6c4d7e50130"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
